{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into a vector database\n",
    "\n",
    "Load chunked context data embedding vectors into an indexed database.  \n",
    "\n",
    "- **Data**: ReadtheDocs documentation pages.\n",
    "- **Vector database**: FAISS and Milvus\n",
    "\n",
    "Demo in progress...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries.\n",
    "import time, os\n",
    "import numpy as np\n",
    "\n",
    "# Import langchain.\n",
    "#!pip install langchain \n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first download the Milvus documentation to a local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download readthedocs page locally.\n",
    "\n",
    "# DOCS_PAGE=\"https://pymilvus.readthedocs.io/en/latest/\"\n",
    "# !echo $DOCS_PAGE\n",
    "\n",
    "# # Specify encoding to handle non-unicode characters in documentation.\n",
    "# !wget -r -A.html -P rtdocs --header=\"Accept-Charset: UTF-8\" $DOCS_PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtdocs/about.html      rtdocs/faq.html        rtdocs/param.html\n",
      "rtdocs/api.html        rtdocs/genindex.html   rtdocs/results.html\n",
      "rtdocs/changes.html    rtdocs/index.html      rtdocs/search.html\n",
      "rtdocs/contribute.html rtdocs/install.html    rtdocs/tutorial.html\n"
     ]
    }
   ],
   "source": [
    "# For now, manually move all .html files directly under rtdocs/\n",
    "!ls rtdocs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb8 in position 585: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/christybergman/Documents/christy_github/ZillizDemos/milvus_onboarding/milvus_load_docs.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christybergman/Documents/christy_github/ZillizDemos/milvus_onboarding/milvus_load_docs.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# TODO:  Our docs HTML header says they are utf-8, but apparently not!\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christybergman/Documents/christy_github/ZillizDemos/milvus_onboarding/milvus_load_docs.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Figure out how to download our ReadtheDocs as utf-8 encoded!\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christybergman/Documents/christy_github/ZillizDemos/milvus_onboarding/milvus_load_docs.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loader \u001b[39m=\u001b[39m ReadTheDocsLoader(\u001b[39m\"\u001b[39m\u001b[39mrtdocs\u001b[39m\u001b[39m\"\u001b[39m, features\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/christybergman/Documents/christy_github/ZillizDemos/milvus_onboarding/milvus_load_docs.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christybergman/Documents/christy_github/ZillizDemos/milvus_onboarding/milvus_load_docs.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m num_documents \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(docs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christybergman/Documents/christy_github/ZillizDemos/milvus_onboarding/milvus_load_docs.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloaded \u001b[39m\u001b[39m{\u001b[39;00mnum_documents\u001b[39m}\u001b[39;00m\u001b[39m documents\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/py310/lib/python3.10/site-packages/langchain/document_loaders/readthedocs.py:66\u001b[0m, in \u001b[0;36mReadTheDocsLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(p, encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 66\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clean_data(f\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m     67\u001b[0m metadata \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(p)}\n\u001b[1;32m     68\u001b[0m docs\u001b[39m.\u001b[39mappend(Document(page_content\u001b[39m=\u001b[39mtext, metadata\u001b[39m=\u001b[39mmetadata))\n",
      "File \u001b[0;32m~/mambaforge/envs/py310/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb8 in position 585: invalid start byte"
     ]
    }
   ],
   "source": [
    "# TODO:  Milvus docs HTML header says they are utf-8, but apparently not!\n",
    "# Figure out how to download Milvus ReadtheDocs as utf-8 encoded!\n",
    "\n",
    "loader = ReadTheDocsLoader(\"rtdocs\", features=\"html.parser\")\n",
    "docs = loader.load()\n",
    "\n",
    "num_documents = len(docs)\n",
    "print(f\"loaded {num_documents} documents\")\n",
    "print(f\"type: {type(docs)}, len: {len(docs)}, type: {type(docs[0])}\")\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 34 documents\n",
      "type: <class 'list'>, len: 34, type: <class 'langchain.schema.document.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='langchain_experimental API Reference¶\\nlangchain_experimental.autonomous_agents¶\\nClasses¶\\nautonomous_agents.autogpt.agent.AutoGPT(...)\\nAgent class for interacting with Auto-GPT.\\nautonomous_agents.autogpt.memory.AutoGPTMemory\\nMemory for AutoGPT.\\nautonomous_agents.autogpt.output_parser.AutoGPTAction(...)\\nAction returned by AutoGPTOutputParser.\\nautonomous_agents.autogpt.output_parser.AutoGPTOutputParser\\nOutput parser for AutoGPT.\\nautonomous_agents.autogpt.output_parser.BaseAutoGPTOutputParser\\nBase Output parser for AutoGPT.\\nautonomous_agents.autogpt.prompt.AutoGPTPrompt\\nPrompt for AutoGPT.\\nautonomous_agents.autogpt.prompt_generator.PromptGenerator()\\nA class for generating custom prompt strings.\\nautonomous_agents.baby_agi.baby_agi.BabyAGI\\nController model for the BabyAGI agent.\\nautonomous_agents.baby_agi.task_creation.TaskCreationChain\\nChain generating tasks.\\nautonomous_agents.baby_agi.task_execution.TaskExecutionChain\\nChain to execute tasks.\\nautonomous_agents.baby_agi.task_prioritization.TaskPrioritizationChain\\nChain to prioritize tasks.\\nautonomous_agents.hugginggpt.hugginggpt.HuggingGPT(...)\\nautonomous_agents.hugginggpt.repsonse_generator.ResponseGenerationChain\\nChain to execute tasks.\\nautonomous_agents.hugginggpt.repsonse_generator.ResponseGenerator(...)\\nautonomous_agents.hugginggpt.task_executor.Task(...)\\nautonomous_agents.hugginggpt.task_executor.TaskExecutor(plan)\\nLoad tools to execute tasks.\\nautonomous_agents.hugginggpt.task_planner.BasePlanner\\nCreate a new model by parsing and validating input data from keyword arguments.\\nautonomous_agents.hugginggpt.task_planner.Plan(steps)\\nautonomous_agents.hugginggpt.task_planner.PlanningOutputParser\\nCreate a new model by parsing and validating input data from keyword arguments.\\nautonomous_agents.hugginggpt.task_planner.Step(...)\\nautonomous_agents.hugginggpt.task_planner.TaskPlaningChain\\nChain to execute tasks.\\nautonomous_agents.hugginggpt.task_planner.TaskPlanner\\nCreate a new model by parsing and validating input data from keyword arguments.\\nFunctions¶\\nautonomous_agents.autogpt.output_parser.preprocess_json_input(...)\\nPreprocesses a string to be parsed as json.\\nautonomous_agents.autogpt.prompt_generator.get_prompt(tools)\\nGenerates a prompt string.\\nautonomous_agents.hugginggpt.repsonse_generator.load_response_generator(llm)\\nautonomous_agents.hugginggpt.task_planner.load_chat_planner(llm)\\nlangchain_experimental.comprehend_moderation¶\\nClasses¶\\ncomprehend_moderation.amazon_comprehend_moderation.AmazonComprehendModerationChain\\nA subclass of Chain, designed to apply moderation to LLMs.\\ncomprehend_moderation.base_moderation.BaseModeration(client)\\ncomprehend_moderation.base_moderation_callbacks.BaseModerationCallbackHandler()\\ncomprehend_moderation.base_moderation_config.BaseModerationConfig\\nCreate a new model by parsing and validating input data from keyword arguments.\\ncomprehend_moderation.base_moderation_config.ModerationIntentConfig\\nCreate a new model by parsing and validating input data from keyword arguments.\\ncomprehend_moderation.base_moderation_config.ModerationPiiConfig\\nCreate a new model by parsing and validating input data from keyword arguments.\\ncomprehend_moderation.base_moderation_config.ModerationToxicityConfig\\nCreate a new model by parsing and validating input data from keyword arguments.\\ncomprehend_moderation.base_moderation_exceptions.ModerationIntentionError([...])\\nException raised if Intention entities are detected.\\ncomprehend_moderation.base_moderation_exceptions.ModerationPiiError([...])\\nException raised if PII entities are detected.\\ncomprehend_moderation.base_moderation_exceptions.ModerationToxicityError([...])\\nException raised if Toxic entities are detected.\\ncomprehend_moderation.intent.ComprehendIntent(client)\\ncomprehend_moderation.pii.ComprehendPII(client)\\ncomprehend_moderation.toxicity.ComprehendToxicity(client)\\nlangchain_experimental.cpal¶\\nClasses¶\\ncpal.constants.Constant(value[,\\xa0names,\\xa0...])\\nEnum for constants used in the CPAL.\\nlangchain_experimental.data_anonymizer¶\\nClasses¶\\ndata_anonymizer.base.AnonymizerBase()\\nBase abstract class for anonymizers. It is public and non-virtual because it allows     wrapping the behavior for all methods in a base class.\\ndata_anonymizer.base.ReversibleAnonymizerBase()\\nBase abstract class for reversible anonymizers.\\ndata_anonymizer.deanonymizer_mapping.DeanonymizerMapping(...)\\nFunctions¶\\ndata_anonymizer.faker_presidio_mapping.get_pseudoanonymizer_mapping([seed])\\nlangchain_experimental.fallacy_removal¶\\nThe Chain runs a self-review of logical fallacies as determined by this paper  categorizing and defining logical fallacies https://arxiv.org/pdf/2212.07425.pdf. Modeled after Constitutional AI and in same format, but applying logical fallacies as generalized rules to remove in output\\nClasses¶\\nfallacy_removal.base.FallacyChain\\nChain for applying logical fallacy evaluations, modeled after Constitutional AI     and in same format, but applying logical fallacies as generalized rules to remove     in output\\nfallacy_removal.models.LogicalFallacy\\nClass for a logical fallacy.\\nlangchain_experimental.generative_agents¶\\nGenerative Agents primitives.\\nClasses¶\\ngenerative_agents.generative_agent.GenerativeAgent\\nAn Agent as a character with memory and innate characteristics.\\ngenerative_agents.memory.GenerativeAgentMemory\\nMemory for the generative agent.\\nlangchain_experimental.graph_transformers¶\\nClasses¶\\ngraph_transformers.diffbot.DiffbotGraphTransformer([...])\\nTransforms documents into graph documents using Diffbot\\'s NLP API.\\ngraph_transformers.diffbot.NodesList()\\nManages a list of nodes with associated properties.\\ngraph_transformers.diffbot.SimplifiedSchema()\\nProvides functionality for working with a simplified schema mapping.\\nFunctions¶\\ngraph_transformers.diffbot.format_property_key(s)\\nlangchain_experimental.llms¶\\nExperimental LLM wrappers.\\nClasses¶\\nllms.anthropic_functions.AnthropicFunctions\\nCreate a new model by parsing and validating input data from keyword arguments.\\nllms.anthropic_functions.TagParser()\\nA heavy-handed solution, but it\\'s fast for prototyping.\\nllms.jsonformer_decoder.JsonFormer\\nJsonformer wrapped LLM using HuggingFace Pipeline API.\\nllms.llamaapi.ChatLlamaAPI\\nCreate a new model by parsing and validating input data from keyword arguments.\\nllms.rellm_decoder.RELLM\\nRELLM wrapped LLM using HuggingFace Pipeline API.\\nFunctions¶\\nllms.jsonformer_decoder.import_jsonformer()\\nLazily import jsonformer.\\nllms.rellm_decoder.import_rellm()\\nLazily import rellm.\\nlangchain_experimental.pal_chain¶\\nImplements Program-Aided Language Models.\\nAs in https://arxiv.org/pdf/2211.10435.pdf.\\nThis is vulnerable to arbitrary code execution:\\nhttps://github.com/hwchase17/langchain/issues/5872\\nClasses¶\\npal_chain.base.PALChain\\nImplements Program-Aided Language Models (PAL).\\npal_chain.base.PALValidation([...])\\nInitialize a PALValidation instance.\\nlangchain_experimental.plan_and_execute¶\\nClasses¶\\nplan_and_execute.agent_executor.PlanAndExecute\\nPlan and execute a chain of steps.\\nplan_and_execute.executors.base.BaseExecutor\\nBase executor.\\nplan_and_execute.executors.base.ChainExecutor\\nChain executor.\\nplan_and_execute.planners.base.BasePlanner\\nBase planner.\\nplan_and_execute.planners.base.LLMPlanner\\nLLM planner.\\nplan_and_execute.planners.chat_planner.PlanningOutputParser\\nPlanning output parser.\\nplan_and_execute.schema.BaseStepContainer\\nBase step container.\\nplan_and_execute.schema.ListStepContainer\\nList step container.\\nplan_and_execute.schema.Plan\\nPlan.\\nplan_and_execute.schema.PlanOutputParser\\nPlan output parser.\\nplan_and_execute.schema.Step\\nStep.\\nplan_and_execute.schema.StepResponse\\nStep response.\\nFunctions¶\\nplan_and_execute.executors.agent_executor.load_agent_executor(...)\\nLoad an agent executor.\\nplan_and_execute.planners.chat_planner.load_chat_planner(llm)\\nLoad a chat planner.\\nlangchain_experimental.prompt_injection_identifier¶\\nHuggingFace Security toolkit.\\nClasses¶\\nprompt_injection_identifier.hugging_face_identifier.HuggingFaceInjectionIdentifier\\nTool that uses deberta-v3-base-injection to detect prompt injection attacks.\\nFunctions¶\\nlangchain_experimental.smart_llm¶\\nGeneralized implementation of SmartGPT (origin: https://youtu.be/wVzuvf9D9BU)\\nClasses¶\\nsmart_llm.base.SmartLLMChain\\nGeneralized implementation of SmartGPT (origin: https://youtu.be/wVzuvf9D9BU)\\nlangchain_experimental.sql¶\\nChain for interacting with SQL Database.\\nClasses¶\\nsql.base.SQLDatabaseChain\\nChain for interacting with SQL Database.\\nsql.base.SQLDatabaseSequentialChain\\nChain for querying SQL database that is a sequential chain.\\nlangchain_experimental.tot¶\\nClasses¶\\ntot.base.ToTChain\\nA Chain implementing the Tree of Thought (ToT).\\ntot.checker.ToTChecker\\nTree of Thought (ToT) checker.\\ntot.controller.ToTController([c])\\nTree of Thought (ToT) controller.\\ntot.memory.ToTDFSMemory([stack])\\nMemory for the Tree of Thought (ToT) chain.\\ntot.prompts.CheckerOutputParser\\nCreate a new model by parsing and validating input data from keyword arguments.\\ntot.prompts.JSONListOutputParser\\nClass to parse the output of a PROPOSE_PROMPT response.\\ntot.thought.Thought\\nCreate a new model by parsing and validating input data from keyword arguments.\\ntot.thought.ThoughtValidity(value[,\\xa0names,\\xa0...])\\ntot.thought_generation.BaseThoughtGenerationStrategy\\nBase class for a thought generation strategy.\\ntot.thought_generation.ProposePromptStrategy\\nPropose thoughts sequentially using a \"propose prompt\".\\ntot.thought_generation.SampleCoTStrategy\\nSample thoughts from a Chain-of-Thought (CoT) prompt.', metadata={'source': 'rtdocs2/api.python.langchain.com/en/latest/experimental_api_reference.html'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Try a different docs page - this one works fine!\n",
    "\n",
    "# DOCS_PAGE=\"https://api.python.langchain.com/en/latest/\"\n",
    "# !echo $DOCS_PAGE\n",
    "\n",
    "# # Specify encoding to handle non-unicode characters in documentation.\n",
    "# !wget -r -A.html -P rtdocs2 --header=\"Accept-Charset: UTF-8\" $DOCS_PAGE\n",
    "\n",
    "loader = ReadTheDocsLoader(\"rtdocs2\", features=\"html.parser\")\n",
    "docs = loader.load()\n",
    "\n",
    "num_documents = len(docs)\n",
    "print(f\"loaded {num_documents} documents\")\n",
    "print(f\"type: {type(docs)}, len: {len(docs)}, type: {type(docs[0])}\")\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking time: 0.011813163757324219\n",
      "type: <class 'list'>, len: 700\n",
      "\n",
      "Looking at a sample chunk...\n",
      "{'source': 'rtdocs2/api.python.langchain.com/en/latest/experimental_api_reference.html'}\n",
      "langchain_experimental API Reference¶\n",
      "langchain_experimental.autonomous_agents¶\n",
      "Classes¶\n",
      "autonomous_\n"
     ]
    }
   ],
   "source": [
    "# Chunk the data using Langchain's RecursiveCharacterTextSplitter.\n",
    "start_time = time.time()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents(\n",
    "    [doc.page_content for doc in docs], \n",
    "    metadatas=[doc.metadata for doc in docs])\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"chunking time: {end_time - start_time}\")\n",
    "# print(f\"type: {type(chunks)}, len: {len(chunks)}, type: {type(chunks[0])}\")\n",
    "print(f\"type: {type(chunks)}, len: {len(chunks)}\") \n",
    "\n",
    "print()\n",
    "print(\"Looking at a sample chunk...\")\n",
    "print(chunks[0].metadata)\n",
    "print(chunks[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://api.python.langchain.com/en/latest/experimental_api_reference.html'}\n",
      "langchain_experimental API Reference¶\n",
      "langchain_experimental.autonomous_agents¶\n",
      "Classes¶\n",
      "autonomous_agents.autogpt.agent.AutoGPT(...)\n",
      "Agent class for interacting with Auto-GPT.\n",
      "autonomous_agents.autogpt.memory.AutoGPTMemory\n",
      "Memory for AutoGPT.\n",
      "autonomous_agents.autogpt.output_parser.AutoGPTAction(...)\n",
      "Action returned by AutoGPTOutputParser.\n",
      "autonomous_agents.autogpt.output_parser.AutoGPTOutputParser\n",
      "Output parser for AutoGPT.\n",
      "autonomous_agents.autogpt.output_parser.BaseAutoGPTOutputParser\n"
     ]
    }
   ],
   "source": [
    "# Clean up the metadata urls\n",
    "for doc in chunks:\n",
    "    new_url = doc.metadata[\"source\"]\n",
    "    new_url = new_url.replace(\"rtdocs2\", \"https:/\")\n",
    "    doc.metadata.update({\"source\": new_url})\n",
    "\n",
    "print(chunks[0].metadata)\n",
    "print(chunks[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk and embed data\n",
    "\n",
    "**First, choose an embedding model** <br>\n",
    "Most tutorials default to the OpenAI embedding model, which costs money.  You don't have to do that.\n",
    "\n",
    "In the code below, we will use an open source SentenceTransformer embedding model, hosted on HuggingFace.  [SentenceTransformers](https://www.sbert.net/) is a python package that can generate text and image embeddings, originating from Sentence-BERT.\n",
    "\n",
    "SentenceTransformers embeddings are called using the HuggingFaceEmbeddings integration. We have also added an alias for SentenceTransformerEmbeddings for users who are more familiar with directly using that package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import torch.\n",
    "#!pip install torch\n",
    "import torch\n",
    "\n",
    "# Initialize torch settings and get DEVICE.\n",
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 415\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.embeddings.huggingface.HuggingFaceEmbeddings"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": DEVICE}\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "type(hf)\n",
    "# type: langchain.embeddings.huggingface.HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: **\n",
    "- Find a better question to ask!\n",
    "- Double-check chunk size and sequence lengths best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "embedding vector length: 768, max_seq_length: 512\n",
      "CHUNK_SIZE: 459, chunk_overlap: 46.0\n"
     ]
    }
   ],
   "source": [
    "# TODO:  How to inspect the embedding model parameters?\n",
    "# In this cell, I'll import directly and inspect.\n",
    "\n",
    "# Import HuggingFace and SentenceTransformers.\n",
    "#!pip install transformers sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load the retriever model from huggingface model hub\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# model_name = \"thenlper/gte-large\"\n",
    "# model_name = \"intfloat/e5-base-v2\"\n",
    "retriever = SentenceTransformer(model_name, device=DEVICE)\n",
    "print(type(retriever))\n",
    "print(retriever)\n",
    "\n",
    "# Define a query for estimation purposes.\n",
    "query = 'Which index does Langchain Milvus default to?'\n",
    "QUERY_LENGTH = len(query)\n",
    "TOKENIZER_EOS = 4 # HuggingFace default is 4 bytes\n",
    "\n",
    "# Save params for later.\n",
    "max_seq_length = retriever.get_max_seq_length() #128\n",
    "CHUNK_SIZE = max_seq_length - QUERY_LENGTH - 2*TOKENIZER_EOS\n",
    "chunk_overlap = np.round(CHUNK_SIZE * 0.10, 0) #19\n",
    "EMBEDDING_LENGTH = retriever.get_sentence_embedding_dimension() #384\n",
    "print(f\"embedding vector length: {EMBEDDING_LENGTH}, max_seq_length: {max_seq_length}\")\n",
    "print(f\"CHUNK_SIZE: {CHUNK_SIZE}, chunk_overlap: {chunk_overlap}\")\n",
    "\n",
    "# model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# embedding vector length: 768, chunk size: 512, chunk_overlap: 51.0\n",
    "\n",
    "# model_name = \"thenlper/gte-large\"\n",
    "# embedding vector length: 1024, chunk size: 512, chunk_overlap: 51.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Define a query for estimation purposes.\n",
    "query = 'Which index does Langchain Milvus default to?'\n",
    "QUERY_LENGTH = len(query)\n",
    "print(QUERY_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embeddings in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed and create vector index\n",
      "FAISS insert time: 60.16475701332092\n",
      "type: <class 'langchain.vectorstores.faiss.FAISS'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x308522440>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload data to FAISS vectorstore.\n",
    "# !python -m pip install faiss-cpu\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "print(\"Embed and create vector index\")\n",
    "start_time = time.time()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding=hf)\n",
    "\n",
    "# Persist the vector store.\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"FAISS insert time: {end_time - start_time}\")\n",
    "print(f\"type: {type(vectorstore)}\")\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try Milvus\n",
    "\n",
    "[In a previous notebook](milvus_connect.ipynb), I showed how to connect to milvus and invoke Python APIs from scratch.\n",
    "\n",
    "The code below uses the [Langchain Milvus](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.milvus.Milvus.html) adapter.  Defaults are:\n",
    "\n",
    "- embedding_function (Embeddings) – Function used to embed the text.\n",
    "- collection_name (str) – Which Milvus collection to use. Defaults to “LangChainCollection”.\n",
    "- connection_args (Optional[dict[str, any]]) – The connection args used for this class comes in the form of a dict.\n",
    "- consistency_level (str) – The consistency level to use for a collection. Defaults to “Session”.\n",
    "- index_params (Optional[dict]) – Which index params to use. Defaults to HNSW/AUTOINDEX depending on service.\n",
    "- search_params (Optional[dict]) – Which search params to use. Defaults to default of index.\n",
    "- drop_old (Optional[bool]) – Whether to drop the current collection. Defaults to False.\n",
    "- primary_field (str) – Name of the primary key field. Defaults to “pk”.\n",
    "- text_field (str) – Name of the text field. Defaults to “text”.\n",
    "- vector_field (str) – Name of the vector field. Defaults to “vector”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed and create vector index\n",
      "vector database insert time: 59.87231993675232\n",
      "type: <class 'langchain.vectorstores.milvus.Milvus'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.milvus.Milvus at 0x30845b8e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload data to Milvus vectorstore.\n",
    "\n",
    "#!pip install pymilvus\n",
    "from langchain.vectorstores import Milvus\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_HOST = \"127.0.0.1\"\n",
    "\n",
    "print(\"Embed and create vector index\")\n",
    "start_time = time.time()\n",
    "vector_store = Milvus.from_documents(\n",
    "    chunks,\n",
    "    embedding=hf,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT},\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"vector database insert time: {end_time - start_time}\")\n",
    "print(f\"type: {type(vector_store)}\")\n",
    "vector_store\n",
    "\n",
    "# Inserting into Milvus, a full-fledged database is just as fast as FAISS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the vectors\n",
    "\n",
    "This is the \"Retrieval\" part of \"RAG\" (Retrieval Augmented Generator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which index does Langchain Milvus default to?\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: {'source': 'https://api.python.langchain.com/en/latest/api_reference.html'}\n",
      "['retrievers.self_query.milvus.MilvusTranslator()\\nTranslate Milvus internal query language elements to valid filters.\\nretrievers.self_query.myscale.MyScaleTranslator([...])\\nTranslate MyScale internal query language elements to valid filters.\\nretrievers.self_query.pinecone.PineconeTranslator()\\nTranslate Pinecone internal query language elements to valid filters.\\nretrievers.self_query.qdrant.QdrantTranslator(...)\\nTranslate Qdrant internal query language elements to valid filters.']\n"
     ]
    }
   ],
   "source": [
    "# RETRIEVAL USING FAISS\n",
    "\n",
    "# Always use the exact same LLM for both doc and query embeddings.\n",
    "query_embedding = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "db = FAISS.load_local(\"faiss_index\", query_embedding)\n",
    "documents = db.similarity_search(query=query, k=1)\n",
    "print(f\"source: {docs[0].metadata}\")\n",
    "print([doc.page_content for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: {'source': 'https://api.python.langchain.com/en/latest/api_reference.html'}\n",
      "['retrievers.self_query.milvus.MilvusTranslator()\\nTranslate Milvus internal query language elements to valid filters.\\nretrievers.self_query.myscale.MyScaleTranslator([...])\\nTranslate MyScale internal query language elements to valid filters.\\nretrievers.self_query.pinecone.PineconeTranslator()\\nTranslate Pinecone internal query language elements to valid filters.\\nretrievers.self_query.qdrant.QdrantTranslator(...)\\nTranslate Qdrant internal query language elements to valid filters.']\n"
     ]
    }
   ],
   "source": [
    "# RETRIEVAL USING MILVUS\n",
    "docs = vector_store.similarity_search(query)\n",
    "print(f\"source: {docs[0].metadata}\")\n",
    "print([doc.page_content for doc in documents])\n",
    "\n",
    "# Retrieval using Milvus takes 0.2s vs 1.2s for FAISS !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the chat answer using retrieved texts.\n",
    "\n",
    "This is the \"Generator\" part of \"RAG\" (Retrieval Augmented Generator).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Author: Christy Bergman\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.12\n",
      "IPython version      : 8.15.0\n",
      "\n",
      "torch       : 2.0.1\n",
      "transformers: 4.33.2\n",
      "pymilvus    : 2.3.0\n",
      "langchain   : 0.0.292\n",
      "\n",
      "conda environment: py310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Props to Sebastian Raschka for this handy watermark.\n",
    "# !pip install watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a 'Christy Bergman' -v -p torch,transformers,pymilvus,langchain --conda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
